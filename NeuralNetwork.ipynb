{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "$$linear(z) = z$$\n",
    "$$relu(z) = max(0, z)$$\n",
    "$$sigmoid(z) = 1 / (1 + e^{-z})$$\n",
    "$$tanh(z) = 2/(1+e^{-2z})-1$$\n",
    "$$softplus(z) = ln(1+e^{z})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_function(z,activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        z : z can either be a number or a numpy array\n",
    "        activation: Python string denoting the activation function to be used in the current layer \n",
    "    Outputs:\n",
    "        Activation function applied to z; if z is an array, the function must be applied to its each element\n",
    "    \"\"\"\n",
    "    if activation == \"linear\":\n",
    "        return z\n",
    "    elif activation == \"relu\":\n",
    "        return z*(z>0)\n",
    "    elif activation == \"sigmoid\":\n",
    "        return 1 / (1 + np.exp(-1*z))\n",
    "    elif activation == \"tanh\":\n",
    "        return (2 / (1 + np.exp(-2*z))) -1\n",
    "    elif activation == \"softplus\":\n",
    "        return np.log(1+ np.exp(z))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Weights\n",
    "The NN will contain $H-1$ hidden layers and an output layer. For a layer $h$, we will have two parameters/variables:\n",
    "  * Weight matrix $W^{[h]}$(of shape (size of current layer, size of previous layer)),\n",
    "  * Bias vector $b^{[h]}$(of shape (size of current layer, 1)),  \n",
    "\n",
    "*   Used np.random.uniform(low, high, shape), with low=0.0 and high=0.01 to initialize the weights $W^{[h]}$ (<a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html#numpy-random-uniform\">np.random.uniform()</a>).\n",
    "\n",
    "*   Initialized biases $b^{[h]}$ with zeros (<a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\">np.zeros()</a>).\n",
    "\n",
    "* For a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3, the input parameter ```layers_sizes``` will be **[5, 4, 3]**. The output of the ```construct_NN()``` function will be  **```[[np.array(shape=(4, 5)), np.array(shape=(4, 1))], [np.array(shape=(3, 4)), np.array(shape=(3, 1))]]```**, with each numpy array initialised as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_NN(layers_sizes):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        layers_sizes: A list (of length H+1) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.  \n",
    "    Outputs:\n",
    "        weights: A list (of length H), where each element is a list - [Wh, bh], representing the  weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "    \"\"\"\n",
    "    low = 0\n",
    "    high = 0.01\n",
    "    H = len(layers_sizes)-1\n",
    "    weights = []\n",
    "    for h in range(H):\n",
    "        w = np.random.uniform(low, high, (layers_sizes[h+1],layers_sizes[h]))\n",
    "        b = np.zeros((layers_sizes[h+1],1))\n",
    "        weights.append([w,b])\n",
    "    return weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "This function carries out computations of a single layer of the neural network:\n",
    " $$ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$$\n",
    " $$ A^{[h]} = g^{[h]}(Z^{[h]}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        A_prev: Input data OR Previous layer's activations of shape (size of previous layer, number of examples)\n",
    "        W: Weight matrix of shape (size of current layer, size of previous layer)\n",
    "        b: Bias vector of shape (size of the current layer, 1)\n",
    "        activation: Python string denoting the activation function to be used in the current layer \n",
    "    Outputs:\n",
    "        A: Current layer's activations of shape (size of current layer, number of examples)\n",
    "        Z: Current layer's pre activation value, i.e. input of activation function\n",
    "    \"\"\"\n",
    "    Z = W.dot(A_prev)+b\n",
    "    A = act_function(Z, activation)  \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Forward Propagation Module\n",
    "* This function iterates over the $H$ layers of the neural network using a for loop, computing the final output of the neural network. \n",
    "* It also outputs a ```layers_cache``` list (of length H), whose each element corresponds to a layer $h$ and is a list[$A^{[h-1]}, W^{[h]}, Z^{[h]}$]. Storing these values now will be helpful while computing gradients n the backward pass.\n",
    "* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```layers_cache``` list will be [[$X or A^{[0]}$, $W^{[1]}$, $Z^{[1]}$], [$A^{[1]}$, $W^{[2]}$, $Z^{[2]}$]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_module(X, weights, activations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: Input data of shape (size of input layer, number of examples)\n",
    "        weights: A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "    Outputs:\n",
    "        AH: Output layer's activations\n",
    "        layers_cache: A list (of length H), where each element is a list [A_prev, W, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer)\n",
    "          and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n",
    "    \"\"\"\n",
    "    layers_cache = []\n",
    "    H = len(weights)\n",
    "    A = X\n",
    "\n",
    "    for h in range(H):\n",
    "        activation = activations[h]\n",
    "        #     print('{}, weight {}'.format(h, weights[h][0].shape))\n",
    "        _,Z = forward_step(A, weights[h][0],weights[h][1], activation, alpha)\n",
    "        layers_cache.append([A,weights[h][0],Z])\n",
    "        A,_ = forward_step(A, weights[h][0],weights[h][1], activation, alpha)\n",
    "\n",
    "    return A, layers_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Cost Function\n",
    "Compute the cross-entropy cost using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}*\\log\\left(a^{[H] (i)}\\right) + (1-y^{(i)})*\\log\\left(1- a^{[H](i)}\\right))$$\n",
    "\n",
    "Compute the mean square error cost using the following formula: $$\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left(y^{(i)} - \\left(a^{[H] (i)}\\right)\\right)^2 $$\n",
    "where  \n",
    "$$m = number \\ of \\ examples$$\n",
    "$$ y^{(i)} = Ground \\ truth \\ labels \\ for \\ i-th \\ example $$\n",
    "$$ a^{[H](i)} = Output \\ layer \\ activations \\ for \\ i-th \\ example $$\n",
    "$$ * \\ denotes \\ elementwise \\ multiplication $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AH, Y,error_function,error_function):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        AH: Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n",
    "        Y: Ground truth \"label\" vector of shape (1, number of examples)\n",
    "        error_function: Python string denoting the error function to be used\n",
    "    Outputs:\n",
    "        cost: Cost of shape () i.e. scalar value\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    if error_function == \"cross_entropy\":\n",
    "        cost = (-1/m)*((np.multiply(Y,np.log(AH)) + np.multiply(1 - Y, np.log(1-AH))).sum())\n",
    "    elif error_function == \"mean_square\"\n",
    "      cost = (1/m)*np.sum(np.square(Y-AH))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Activation Backward Propagation\n",
    "This function computes  $dZ^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[h]}}$, given  $dA^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h]}}$ and $Z^{[h]}$as inputs. It is based on the following chain rule from calculus:\n",
    "$$ dZ^{[h]} = dA^{[h]} * \\frac{\\partial \\mathcal{A^{[h]}} }{\\partial Z^{[h]}}$$ <br>\n",
    "\n",
    "Note the following rules for different activation functions:\n",
    "$$ \\frac{\\partial \\mathcal{(relu(z))} }{\\partial z} =   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      1 & z>=0 \\\\\n",
    "      0& z < 0 \\\\\n",
    "\\end{array} \n",
    "\\right.  $$<br>\n",
    "$$\\frac{\\partial \\mathcal{(sigmoid(z))} }{\\partial z} = sigmoid(z) * (1-sigmoid(z))$$<br>\n",
    "$$\\frac{\\partial \\mathcal{(tanh(z))} }{\\partial z} = 1-tanh(z)*tanh(z)$$<br>\n",
    "$$\\frac{\\partial \\mathcal{(softplus(z))} }{\\partial z} = sigmoid(z) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_backward(dA, Z,activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA: Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n",
    "        Z: Pre activation value, of shape(size of current layer, number of examples)\n",
    "        activation: Python string denoting the activation function to be used in the current layer \n",
    "    Outputs:\n",
    "        dZ: Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n",
    "    \"\"\"\n",
    "    if activation == \"linear\":\n",
    "        dZ = dA\n",
    "    elif activation == \"relu\":\n",
    "        dZ = dA * (Z>=0)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = dA * sigmoid(Z)*(1- sigmoid(Z))\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = np.multiply(dA, (1- tanh(Z)*tanh(Z)))\n",
    "    elif activation == \"softplus\":\n",
    "        dZ = dA * sigmoid(Z)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation Step\n",
    "* The following function computes $dA^{[h-1]}, dW^{[h]}$ and $db^{[h]}$, given $dA^{[h]}, Z^{[h]}, W^{[h]}$ and $A^{[h-1]}$ as inputs.  \n",
    "* We can calculate $dZ^{[h]}$ using $dA^{[h]}$ and $Z^{[h]}$ as inputs to the ```activation_backward()``` function defined above.  \n",
    "* The three outputs $(dA^{[h-1]}, dW^{[h]}, db^{[h]})$ are computed using $dZ^{[h]}, W^{[h]}$ and $A^{[h-1]}$ using the below equations(using the fact that $ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$):  \n",
    "  - $ dA^{[h-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h-1]}} = W^{[h] T} dZ^{[h]}$\n",
    "  - $ dW^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[h]}} = \\frac{1}{m} dZ^{[h]} A^{[h-1] T}$\n",
    "  - $ db^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[h](i)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, layer_cache, activation):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        dA: Gradient of the cost function w.r.t. the post activation value A for current layer\n",
    "        layer_cache: A list [A_prev, W, Z] for current layer, where A_prev is activation of prev layer, W is weight matrix of current layer, and Z is pre-activation value for current layer\n",
    "        activation: Python string denoting the activation function used in the current layer\n",
    "    Outputs:\n",
    "        dA_prev: Gradient of the cost function w.r.t. the activation (of the previous layer h-1), of same shape as A_prev\n",
    "        dW: Gradient of the cost function w.r.t. W (current layer l), of same shape as W\n",
    "        db: Gradient of the cost function w.r.t. b (current layer l), of same shape as b\n",
    "    \"\"\"\n",
    "    A_prev = layer_cache[0]\n",
    "    W = layer_cache[1]\n",
    "    Z = layer_cache[2]\n",
    "\n",
    "    m = Z.shape[1]\n",
    "    dZ = activation_backward(dA,Z,activation,alpha)\n",
    "    dA_prev = np.matmul((W.T),dZ)\n",
    "    dW = np.matmul(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis =1,keepdims = True)/m\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation Module \n",
    "* This function computes the gradients of loss function w.r.t. all the weights of the neural network.\n",
    "* To backpropagate through this network, we know that the output is, $A^{[H]} = sigmoid(Z^{[H]})$. Your code thus needs to compute $ dA^{[h]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[H]}}$. To do so, use this formula:\n",
    "$$ dA^{[h]} = -(Y / A^{[h]} - (1-Y)/(1-A^{[h]}))$$\n",
    "where $ \"/\" \\ denotes \\ elementwise \\ division$\n",
    "* The output of this function is a list ```gradients``` of length $H$, whose each element is a list [$dW^{[h]}, db^{[h]}$].\n",
    "* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```gradients``` will be [[$dW^{[1]}, db^{[1]}$], [$dW^{[2]}, db^{[2]}$]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.5 Marks\n",
    "def backward_module(AH, Y, layers_cache, activations):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        AH: Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n",
    "        Y: Ground truth \"label\" vector of shape (1, number of examples)\n",
    "        layers_cache - A list (of length H), where each element is a list [A_prev, W, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer)\n",
    "          and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n",
    "        activations - A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "\n",
    "    Outputs:\n",
    "        gradients: A list (of length H), where each element is a list [dW, db], representing the values dW (gradient of cost function w.r.t. W) and  \n",
    "          db (gradient of cost function w.r.t. b)\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    H = len(layers_cache) # the number of layers\n",
    "    m = AH.shape[1]\n",
    "    Y = Y.reshape(AH.shape) \n",
    "    dAH = -(Y/AH - (1-Y)/(1-AH))\n",
    "    dA = dAH\n",
    "    for i in reversed(range(H)):\n",
    "        dA_prev, dW, db = backward_step(dA,layers_cache[i],activations[i],alpha)\n",
    "        gradients.insert(0,(dW,db))\n",
    "        dA = dA_prev\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent Optimizer\n",
    "Now that we have calculated the gradients of the loss function w.r.t. all the weights of the network, update the weights of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[h]} = W^{[h]} - \\alpha \\text{ } dW^{[h]} $$\n",
    "$$ b^{[h]} = b^{[h]} - \\alpha \\text{ } db^{[h]} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate.   \n",
    "After computing the updated weights, store them back in the weights list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, gradients, lr):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        weights: A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n",
    "        gradients: A list (of length H), where each element is a list - [dWh, dbh], representing the gradients of cost function w.r.t. weights matrix and bias vector \n",
    "          of layer h respectively (Both dWh and dbh are numpy arrays).\n",
    "        lr: Learning Rate.\n",
    "    Outputs:\n",
    "        weights: A list (of length H), containing the updated weights.\n",
    "    \"\"\"\n",
    "    H = len(weights)\n",
    "    for h in range(H):\n",
    "        weights[h][0] -= lr*gradients[h][0]\n",
    "        weights[h][1] -= lr*gradients[h][1]\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a H-layer Neural Network:\n",
    "This function performs the following steps:  \n",
    "1. Initialize the weights of the neural network .\n",
    "2. Perform forward propagation on input argument ```X```.\n",
    "3. Compute cross entropy loss function.\n",
    "4. Perform backward propagation to compute gradients of loss function w.r.t. all the weights of the network.\n",
    "5. Use the gradients computed in step 4 to update the weights of the network.\n",
    "6. Append cost's value to the list ```epoch_wise_costs```.\n",
    "7. Print (epoch, cross entropy cost's value) for epoch = 0, 100, 200, ...\n",
    "8. Repeat steps 2-7 for ```epochs```(input argument to below function) number of iterations.\n",
    "9. Return the weights of the neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer_NN(X, Y, layers_sizes, activations, epochs=200, lr=0.001, error_function=\"mean_square\"):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: Input data, numpy array of shape (number_of_features_in_input_data, number of examples)\n",
    "        Y: Ground truth \"label\" vector of shape (1, number of examples)\n",
    "        layers_sizes: A list (of length H+1) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.\n",
    "        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        epochs: number of epochs of the training loop\n",
    "        lr: learning rate of the gradient descent update rule\n",
    "        error_function: Python string denoting the error function to be used\n",
    "    Outputs:\n",
    "        weights - weights learnt by the model (can be used for prediction). A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h.\n",
    "    \"\"\"\n",
    "    epoch_wise_costs = []\n",
    "    \n",
    "    weights = construct_NN(layers_sizes)\n",
    "    for epoch in range(epochs):\n",
    "        AH, layers_cache = forward_module(X, weights, activations, alpha)\n",
    "        cost = cross_entropy_cost(AH, Y,error_function)\n",
    "        gradients = backward_module(AH, Y, layers_cache, activations, alpha)\n",
    "        update_weights(weights, gradients, lr)\n",
    "        epoch_wise_costs.append(cost)\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Cost after epoch %i: %f\" %(epoch, cost))\n",
    "    \n",
    "    plt.plot(np.array(epoch_wise_costs))\n",
    "    plt.ylabel('Cost Function')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.title(\"Training Curve\")\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Currently Binary Prediction. Need to change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, weights, activations, alpha=0.1, mode=\"Test\",NN_type):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        X: Input data/examples you would like to predict\n",
    "        Y: Ground truth labels corresponding to input data X\n",
    "        weights: weights of the trained model\n",
    "        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n",
    "        mode: Test/Training mode\n",
    "        NN_type: Normal/ Wavelet Architecture\n",
    "    Outputs:\n",
    "        p: predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    p = np.zeros(m)\n",
    "    AH, _ = forward_module(X, weights, activations, alpha)\n",
    "    AH = np.squeeze(AH)\n",
    "    pos_pred = AH > 0.5\n",
    "    p[pos_pred] = 1\n",
    "    print(NN_type +\" Neural Network: \" + mode + \" Accuracy: \"  + str(np.sum(p == Y)/m))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_sizes = [12, 25, 1]\n",
    "epochs = 10000\n",
    "lr = 0.03\n",
    "error_function = \"mean_square\"\n",
    "activations_normal = [\"leaky_relu\", \"sigmoid\"]\n",
    "activations_wavelet = [] ###DO\n",
    "weights_normal = H_layer_NN(X_train, Y_train, layers_sizes, activations_normal, epochs, lr, error_function)\n",
    "weights_wavelet = H_layer_NN(X_train, Y_train, layers_sizes, activations_wavelet, epochs, lr, error_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions_normal = predict(X_train, Y_train, weights_normal, activations_normal, mode=\"Train\",\"Normal\")\n",
    "test_predictions_normal = predict(X_test, Y_test, weights_normal, activations_normal, mode=\"Test\", \"Normal\")\n",
    "\n",
    "training_predictions_wavelet = predict(X_train, Y_train, weights_wavelet, activations_wavelet, mode=\"Train\",\"Wavelet\")\n",
    "test_predictions_wavelet = predict(X_test, Y_test, weights_wavelet, activations_wavelet, mode=\"Test\", \"Wavelet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "* Wavelet Functions\n",
    "* Fix Prediction Functions\n",
    "* Set Model Architecture\n",
    "* Train and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
