{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"NeuralNetwork_Aashish.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"1p9RuSk8AWo7","colab_type":"code","outputId":"8b980ecb-9fcc-46e6-bb39-eee4e452549b","executionInfo":{"status":"ok","timestamp":1574403568223,"user_tz":-330,"elapsed":2175,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oNNnAcU01Xu9","colab_type":"code","outputId":"1866f1b4-385a-4c83-a90d-cadcd3216105","executionInfo":{"status":"ok","timestamp":1574403572433,"user_tz":-330,"elapsed":6290,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["!pip install numpy==1.16.1\n","import numpy as np\n","import math\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: numpy==1.16.1 in /usr/local/lib/python3.6/dist-packages (1.16.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ad-Nbv4IU77y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":82},"outputId":"18f46441-cbaa-4988-93c0-bfc1a258176a","executionInfo":{"status":"ok","timestamp":1574403572437,"user_tz":-330,"elapsed":6249,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}}},"source":["w = np.load('/gdrive/My Drive/ADSP Project/Weights/trained_normal_4_softmax_mean_square.npy')\n","print(np.any(np.isnan(w[0][0])) or np.any(np.isinf(w[0][0])), w[0][0].shape)\n","print(np.any(np.isnan(w[0][1])) or np.any(np.isinf(w[0][1])), w[0][1].shape)\n","print(np.any(np.isnan(w[1][0])) or np.any(np.isinf(w[1][0])), w[1][0].shape)\n","print(np.any(np.isnan(w[1][1])) or np.any(np.isinf(w[1][1])), w[1][1].shape)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["False (17, 8)\n","False (17, 1)\n","False (6, 17)\n","False (6, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ExEWcPtu1XvD","colab_type":"text"},"source":["# Data Acquisition\n","Dataset Description:\n","* 6 hand movements captured. \n","* 3000 . \n","* Two forearm surface EMG electrodes at Flexor Capri Ulnaris and Extensor Capri Radialis\n","* Sampling rate of 500 Hz.\n","* Filtered via a Butterworth Band Pass Filter with low and high cut-off at 15Hz and 500 Hz. Notch filter at 50 Hz.\n","\n","<center><img src=\"/gdrive/My Drive/ADSP Project/Data/grasps_en.PNG\" width=\"360\" height=\"360\"></center>"]},{"cell_type":"markdown","metadata":{"id":"tMGPp7HH1XvE","colab_type":"text"},"source":["### Data Loading\n","\n","5 subjects, 6 classes, 30 repetitions --> 900 data samples"]},{"cell_type":"markdown","metadata":{"id":"FcmKA3z-1XvF","colab_type":"text"},"source":["### Note:\n","* To not include the effect of bias, the forward module has to be changed, in this case, even though the bias is being updated, it's effect is not seen in the model. Basically a dummy calculation"]},{"cell_type":"code","metadata":{"id":"6FjYKFMt1XvG","colab_type":"code","outputId":"a2bfa23e-4aae-4a6f-cf82-8aa1f4aa5f8c","executionInfo":{"status":"ok","timestamp":1574403574209,"user_tz":-330,"elapsed":7989,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}},"colab":{"base_uri":"https://localhost:8080/","height":147}},"source":["wavelets = [\"bior3.5\",\"bior1.5\",\"bior3.9\",\"coif3\",\"coif5\",\"db2\",\"db9\",\"haar\",\"sym3\",\"sym5\",\"sym7\"]\n","wave_type = 4\n","filename = \"/gdrive/My Drive/ADSP Project/Data/CSV/\" + str(wavelets[wave_type]) + \"_target.csv\"\n","print(filename)\n","Y = pd.read_csv(filename, header=None)\n","Y = np.array(Y)\n","print(Y.shape)\n","\n","filename = \"/gdrive/My Drive/ADSP Project/Data/CSV/\" + str(wavelets[wave_type]) + \"_ADDD.csv\"\n","print(filename)\n","X = pd.read_csv(filename, header=None)\n","X = np.array(X)\n","print(X.shape)\n","\n","N = X.shape[1]\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size = 0.3, random_state = 0,shuffle = True)\n","\n","\n","X_train = X_train.T\n","X_test = X_test.T\n","Y_train = Y_train.T\n","Y_test = Y_test.T\n","\n","X_MAX = 1\n","X_MIN = -1\n","Xmin = np.min(X_train,axis=1)\n","Xmin = Xmin.reshape(Xmin.shape[0],1)\n","Xmax = np.max(X_train,axis=1)\n","Xmax = Xmax.reshape(Xmax.shape[0],1)\n","\n","X_train = ((X_MAX - X_MIN) * (X_train - Xmin)/(Xmax-Xmin)) + Xmin\n","X_test = ((X_MAX - X_MIN) * (X_test - Xmin)/(Xmax-Xmin)) + Xmin\n","\n","print(np.any(np.isnan(X_train)) or np.any(np.isinf(X_train)),X_train.shape)\n","print(np.any(np.isnan(X_test)) or np.any(np.isinf(X_test)),X_test.shape)\n","print(np.any(np.isnan(Y_train)) or np.any(np.isinf(Y_train)),Y_train.shape)\n","print(np.any(np.isnan(Y_test)) or np.any(np.isinf(Y_test)),Y_test.shape)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/gdrive/My Drive/ADSP Project/Data/CSV/coif5_target.csv\n","(6, 900)\n","/gdrive/My Drive/ADSP Project/Data/CSV/coif5_ADDD.csv\n","(8, 900)\n","False (8, 630)\n","False (8, 270)\n","False (6, 630)\n","False (6, 270)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mV87Xkn31XvL","colab_type":"text"},"source":["### Activation Functions\n","\n","$$linear(z) = z$$\n","$$relu(z) = max(0, z)$$\n","$$sigmoid(z) = 1 / (1 + e^{-z})$$\n","$$tanh(z) = 2/(1+e^{-2z})-1$$\n","$$softplus(z) = ln(1+e^{z})$$\n","$$Gaussian(z) = -z*e^{\\frac{z^2}{2}}$$\n","$$MexicanHat(z) = (1-z^2)*e^{\\frac{z^2}{2}}$$\n","$$MorletI(z) =0 cos(1.75z)*e^{\\frac{z^2}{2}}$$\n","$$MorletII(z) = cos(5z)*e^{\\frac{z^2}{2}}$$\n"]},{"cell_type":"code","metadata":{"id":"VU_41GNu1XvO","colab_type":"code","colab":{}},"source":["def act_function(z,activation):\n","    \"\"\"\n","    Inputs:\n","        z : z can either be a number or a numpy array\n","        activation: Python string denoting the activation function to be used in the current layer \n","    Outputs:\n","        Activation function applied to z; if z is an array, the function must be applied to its each element\n","    \"\"\"\n","    if activation == \"linear\":\n","        return z\n","    elif activation == \"relu\":\n","        return z*(z>0)\n","    elif activation == \"sigmoid\":\n","        return 1 / (1 + np.exp(-1*z))\n","    elif activation == \"tanh\":\n","        return (2 / (1 + np.exp(-2*z))) -1\n","    elif activation == \"softplus\":\n","        return np.log(1+ np.exp(z))\n","    elif activation == \"softmax\":\n","        e = z - np.max(z)\n","        return np.exp(e)/np.sum(np.exp(e))\n","    elif activation == \"gaussian_wavelet\":\n","        return -z*np.exp(-0.5*(z**2))\n","    elif activation == \"mexican_hat_wavelet\":\n","        return (1-(z**2))*np.exp(-0.5*(z**2))\n","    elif activation == \"morlet_wavelet_1\":\n","        return np.cos(1.75*z)*np.exp(-0.5*(z**2))\n","    elif activation == \"morlet_wavelet_2\":\n","        return np.cos(5*z)*np.exp(-0.5*(z**2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNRb7Dn91XvU","colab_type":"text"},"source":["### Initializing the Weights\n","The WNN will contain $H=1$ hidden layer and an output layer. For a layer $h$, we will have two parameters/variables:\n","  * Weight matrix $W^{[h]}$(of shape (size of current layer, size of previous layer)),\n","  * Bias vector $bias^{[h]}$(of shape (size of current layer, 1), \n","  * Wavelet scaling factor $a^{[h]}$(of shape (size of current layer, 1),\n","  * Wavelet translation factor $b^{[h]}$(of shape (size of current layer, 1),\n","\n","*   Used np.random.uniform(low, high, shape), with low=0.0 and high=0.01 to initialize the weights $W^{[h]}$ (<a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html#numpy-random-uniform\">np.random.uniform()</a>).a\n","\n","*   Used np.random.uniform(low, high, shape), with low=0.0 and high=5 to initialize the scaling factor $a^{[h]}$ (<a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html#numpy-random-uniform\">np.random.uniform()</a>).\n","\n","*   Used np.random.uniform(low, high, shape), with low=-10.0 and high=10 to initialize the translation factor $b^{[h]}$ (<a href=\"https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.uniform.html#numpy-random-uniform\">np.random.uniform()</a>).\n","\n","*   Initialized biases $b^{[h]}$ with zeros (<a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\">np.zeros()</a>).\n","\n","* **For Normal Neural Network a is initialized to 1 and b to 0**\n","\n","\n","* For a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3, the input parameter ```layers_sizes``` will be **[5, 4, 3]**. The output of the ```construct_WNN()``` function will be  **```[[np.array(shape=(4, 5)), np.array(shape=(4, 1))], np.array(shape=(4, 1))], np.array(shape=(4, 1))], [np.array(shape=(3, 4)), np.array(shape=(3, 1))]]```**, with each numpy array initialised as mentioned above.`"]},{"cell_type":"code","metadata":{"id":"xlkhUIN41XvX","colab_type":"code","colab":{}},"source":["def construct_WNN(layers_sizes,weight_types):\n","    \"\"\"\n","    Inputs:\n","        layers_sizes: A list (of length H+2) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.  \n","        weight_types = A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        weights: A list (of length H), where each element is a list - [Wh, biash,ah,bh], representing the  weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n","    \"\"\"\n","    low_W = 0\n","    high_W = 0.01\n","    low_a = 0\n","    high_a = 5\n","    low_b = -10\n","    high_b = 10\n","    H = len(layers_sizes)-2\n","    weights = []\n","    for h in range(H+1):\n","        w = np.random.uniform(low_W, high_W, (layers_sizes[h+1],layers_sizes[h]))\n","        bias = np.zeros((layers_sizes[h+1],1))\n","        if(weight_types[h] == 'Normal'):\n","            weights.append([w,bias])\n","        elif(weight_types[h] == 'Wavelet'):\n","            a = np.random.uniform(low_a, high_a, (layers_sizes[h+1],1))\n","            b = np.random.uniform(low_b, high_b, (layers_sizes[h+1],1))\n","            weights.append([w,bias,a,b])\n","    return weights "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhVPC1W61Xva","colab_type":"code","colab":{}},"source":["init_weights = construct_WNN([8, 17, 6],[\"Normal\",\"Normal\"])\n","np.save('/gdrive/My Drive/ADSP Project/initial.npy',init_weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBuBz-f71Xvf","colab_type":"code","colab":{}},"source":["low_W = 0\n","high_W = 0.01\n","low_a = 0\n","high_a = 5\n","low_b = -10\n","high_b = 10\n","wight_wavelet = np.load('/gdrive/My Drive/ADSP Project/initial.npy')\n","weight_wavelet = []\n","for h in range(2):\n","    w = wight_wavelet[h][0]\n","    bias = wight_wavelet[h][1]\n","    if(h == 1):\n","        weight_wavelet.append([w,bias])\n","    else:\n","        a = np.random.uniform(low_a, high_a, (layers_sizes[h+1],1))\n","        b = np.random.uniform(low_b, high_b, (layers_sizes[h+1],1))\n","        weight_wavelet.append([w,bias,a,b])\n","np.save('/gdrive/My Drive/ADSP Project/init_wavelet.npy',weight_wavelet)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bs_59g9V1Xvi","colab_type":"text"},"source":["### Forward Propagation\n","This function carries out computations of a single layer of the neural network:\n"," $$ Z^{[h]} = W^{[h]}.A^{[h-1]}+bias^{[h]}$$\n"," $$ A^{[h]} = g^{[h]}(Z^{[h]}) $$"]},{"cell_type":"code","metadata":{"id":"vLKIPK7r1Xvj","colab_type":"code","colab":{}},"source":["def forward_step(A_prev, weight, activation,take_bias,weight_type):\n","    \"\"\"\n","    Inputs:\n","        A_prev: Input data OR Previous layer's activations of shape (size of previous layer, number of examples)\n","        W: List of trainables\n","        activation: Python string denoting the activation function to be used in the current layer \n","        take_bias: True -> Take bias, False -> No Bias\n","        weight_type = A string denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        A: Current layer's activations of shape (size of current layer, number of examples)\n","        Z: Current layer's pre activation value, i.e. input of activation function\n","    \"\"\"\n","    if(weight_type == 'Normal'):\n","        W = weight[0]\n","        bias = weight[1]\n","        a=None\n","        b=None\n","        if(take_bias):\n","            Z = W.dot(A_prev)+bias\n","        else:\n","            Z = W.dot(A_prev)\n","    elif(weight_type == 'Wavelet'):\n","        W = weight[0]\n","        bias = weight[1]\n","        a = weight[2]\n","        b = weight[3]\n","        if(take_bias):\n","            Z = W.dot(A_prev)+bias\n","        else:\n","            Z = W.dot(A_prev)\n","        Z = (Z-b)/a\n","        \n","    A = act_function(Z, activation)\n","    flag1 = (np.any(np.isnan(A)) or np.any(np.isinf(A)))\n","    flag2 = (np.any(np.isnan(Z)) or np.any(np.isinf(A)))\n","    if(flag1 or flag2): \n","        print('W',W)\n","        print('bias',bias)\n","        print('a',a)\n","        print('b',b)\n","        print('Z',Z)\n","        print('A',A)\n","        raise NameError('Error!')\n","    return A, Z"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Q_Aacc11Xvl","colab_type":"text"},"source":["###  Forward Propagation Module\n","* This function iterates over the $H$ layers of the neural network using a for loop, computing the final output of the neural network. \n","* It also outputs a ```layers_cache``` list (of length H), whose each element corresponds to a layer $h$ and is a list[$A^{[h-1]}, W^{[h]}, Z^{[h]}$]. Storing these values now will be helpful while computing gradients n the backward pass.\n","* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```layers_cache``` list will be [[$X or A^{[0]}$, $W^{[1]}$, $Z^{[1]}$], [$A^{[1]}$, $W^{[2]}$, $Z^{[2]}$]]."]},{"cell_type":"code","metadata":{"id":"QHYdcgDu1Xvm","colab_type":"code","colab":{}},"source":["def forward_module(X, weights, activations,take_bias,weight_types):\n","    \"\"\"\n","    Inputs:\n","        X: Input data of shape (size of input layer, number of examples)\n","        weights: A list (of length H), where each element is a list -[Wh, biash] OR [Wh, biash,ah,bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n","        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n","        take_bias: Take bias, False -> No Bias\n","        weight_types = A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        AH: Output layer's activations\n","        layers_cache: A list (of length H), where each element is a list [A_prev, W, a, b, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer),a (scaling factor of the layer), b (translation factor of the layer)\n","          and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n","    \"\"\"\n","    layers_cache = []\n","    H = len(weights)\n","    A = X\n","\n","    for h in range(H):\n","        activation = activations[h]\n","        #     print('{}, weight {}'.format(h, weights[h][0].shape))\n","        _,Z = forward_step(A, weights[h],activation,take_bias,weight_types[h])\n","        if(weight_types[h] == 'Wavelet'):\n","            layers_cache.append([A,weights[h][0],weights[h][2],weights[h][3],Z]) #A_prev, weight, a, b, pre-activation output\n","        elif(weight_types[h] == 'Normal'):\n","            layers_cache.append([A,weights[h][0],None,None,Z]) \n","        A,_ = forward_step(A, weights[h],activation,take_bias,weight_types[h])\n","\n","    return A, layers_cache"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6jsJb6w51Xvo","colab_type":"text"},"source":["###  Cost Function\n","Compute the mean square error cost using the following formula: $$\\frac{1}{m}\\left(\\frac{1}{2} \\sum\\limits_{i = 1}^{m} \\left(y^{(i)} - \\left(a^{[H] (i)}\\right)\\right)^2\\right) $$\n","\n","Compute the cross entropy error cost using the following formula: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left(y^{(i)}*\\log\\left(a^{[H] (i)}\\right)\\right)$$\n","\n","where  \n","$$m = number \\ of \\ examples$$\n","$$ y^{(i)} = Ground \\ truth \\ labels \\ for \\ i-th \\ example $$\n","$$ a^{[H](i)} = Output \\ layer \\ activations \\ for \\ i-th \\ example $$\n","$$ * \\ denotes \\ elementwise \\ multiplication $$\n"]},{"cell_type":"code","metadata":{"id":"b8G1lQNS1Xvp","colab_type":"code","colab":{}},"source":["def cost_function(AH, Y,error_function):\n","    \"\"\"\n","    Inputs:\n","        AH: Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n","        Y: Ground truth \"label\" vector of shape (1, number of examples)\n","        error_function: mean_square / cross_entropy\n","    Outputs:\n","        cost: Cost of shape () i.e. scalar value\n","    \"\"\"\n","    m = Y.shape[1]\n","    if(error_function == \"mean_square\"):\n","        cost = (1/(2*m))*np.sum(np.square(Y-AH))\n","    elif(error_function == \"cross_entropy\"):\n","        cost = (-1/m)*(np.sum((np.multiply(Y,np.log(AH)))) )\n","    \n","    return cost"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WieNjHjl1Xvr","colab_type":"text"},"source":["###  Activation Backward Propagation\n","This function computes  $dZ^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[h]}}$, given  $dA^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h]}}$ and $Z^{[h]}$as inputs. It is based on the following chain rule from calculus:\n","$$ dZ^{[h]} = dA^{[h]} * \\frac{\\partial \\mathcal{A^{[h]}} }{\\partial Z^{[h]}}$$\n","\n","<br> For Wavelet: <br>\n","$$ dZ'^{[h]} = dA^{[h]} * \\frac{\\partial \\mathcal{A^{[h]}} }{\\partial Z'^{[h]}}$$\n","\n","Note the following rules for different activation functions:\n","$$ \\frac{\\partial \\mathcal{(relu(z))} }{\\partial z} =   \\left\\{\n","\\begin{array}{ll}\n","      1 & z>=0 \\\\\n","      0& z < 0 \\\\\n","\\end{array} \n","\\right.  $$<br>\n","$$\\frac{\\partial \\mathcal{(sigmoid(z))} }{\\partial z} = sigmoid(z) * (1-sigmoid(z))$$<br>\n","$$\\frac{\\partial \\mathcal{(tanh(z))} }{\\partial z} = 1-tanh(z)*tanh(z)$$<br>\n","$$\\frac{\\partial \\mathcal{(softplus(z))} }{\\partial z} = sigmoid(z) $$<br>\n","$$\\frac{\\partial \\mathcal{(Gaussian Wavelet(z))} }{\\partial z} = -Mexican Hat(z) $$<br>\n","$$\\frac{\\partial \\mathcal{(Mexican Hat Wavelet(z))} }{\\partial z} = Gaussian(z) * (1-z^2) $$<br>\n","$$\\frac{\\partial \\mathcal{(Morelet Wavelet I(z))} }{\\partial z} = -e^{-\\frac{t^2}{2}}*\\left(1.75*sin(1.75t) + t*cos(1.75t)\\right) $$<br>\n","$$\\frac{\\partial \\mathcal{(Morlet Wavelet II(z))} }{\\partial z} = e^{-\\frac{t^2}{2}}*\\left(5*sin(5t) + t*cos(5t)\\right) $$<br>"]},{"cell_type":"code","metadata":{"id":"jzdPrqeZ1Xvs","colab_type":"code","colab":{}},"source":["def act_backward(dA, Z,activation):\n","    \"\"\"\n","    Inputs:\n","        dA: Gradient of the cost function w.r.t. the post activation value A, of shape(size of current layer, number of examples)\n","        Z: Pre activation value, of shape(size of current layer, number of examples)\n","        activation: Python string denoting the activation function to be used in the current layer \n","    Outputs:\n","        dZ: Gradient of the cost function w.r.t. Z, of shape(size of current layer, number of examples)\n","    \"\"\"\n","    if activation == \"linear\":\n","        dZ = dA\n","    elif activation == \"relu\":\n","        dZ = dA*(Z>=0)\n","    elif activation == \"sigmoid\":\n","        dZ = dA*act_function(Z,'sigmoid')*(1- act_function(Z,'sigmoid'))\n","    elif activation == \"tanh\":\n","        dZ = np.multiply(dA,(1- act_function(Z,'tanh')*act_function(Z,'tanh')))\n","    elif activation == \"softplus\":\n","        dZ = dA*act_function(Z,'sigmoid')\n","    elif activation == \"softmax\":\n","        dZ = dA #dA*(act_function(Z,'softmax')-1)\n","    elif activation == 'gaussian_wavelet':\n","        dZ = dA*(-1*act_function(Z,'mexican_hat_wavelet'))\n","    elif activation == 'mexican_hat_wavelet':\n","        dZ = dA*(np.multiply(act_function(Z,'gaussian_wavelet'), (3-Z**2)))\n","    elif activation == 'morlet_wavelet_1':\n","        dZ = dA*(np.multiply(np.exp(-0.5*(z**2)), (1.75*np.sin(1.75*Z)+ Z*np.cos(1.75*Z))))\n","    elif activation == 'morlet_wavelet_2':\n","        dZ = dA*(np.multiply(np.exp(-0.5*(z**2)), (5*np.sin(5*Z)+ Z*np.cos(5*Z))))\n","    return dZ\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMzXc8os1Xvx","colab_type":"text"},"source":["### Backward Propagation Step\n","* The following function computes $dA^{[h-1]}, dW^{[h]}$ and $db^{[h]}$, given $dA^{[h]}, Z^{[h]}, W^{[h]}$ and $A^{[h-1]}$ as inputs.  \n","* We can calculate $dZ^{[h]}$ using $dA^{[h]}$ and $Z^{[h]}$ as inputs to the ```activation_backward()``` function defined above.  \n","* The three outputs $(dA^{[h-1]}, dW^{[h]}, dbias^{[h]})$ are computed using $dZ^{[h]}, W^{[h]}$ and $A^{[h-1]}$ using the below equations(using the fact that $ Z^{[h]} = W^{[h]}.A^{[h-1]}+b^{[h]}$): \n"," <br> If Normal:<br>\n","  - $ dA^{[h-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h-1]}} = W^{[h] T} dZ^{[h]}$\n","  - $ dW^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[h]}} = \\frac{1}{m} dZ^{[h]} A^{[h-1] T}$\n","  - $ dbias^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial bias^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[h](i)}$\n","  <br>If Wavelet:<br>\n","  - $ dA^{[h-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[h-1]}} = W^{[h] T} dZ'^{[h]}*\\frac{1}{a^{[h](i)}}$\n","  - $ dW^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[h]}} = \\frac{1}{m} dZ'^{[h]} A^{[h-1] T}*\\frac{1}{a^{[h](i)}}$\n","  - $ dbias^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial bias^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ'^{[h](i)}*\\frac{1}{a^{[h](i)}}$\n","  - $ da^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial a^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ'^{[h](i)}*(-1)*\\frac{Z'^{[H](i)}}{a^{[h](i)}}$\n","  - $ db^{[h]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[h]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ'^{[h](i)}*(-1)\\frac{1}{a^{[h](i)}}$\n"]},{"cell_type":"code","metadata":{"id":"hx9sF7IZ1Xvy","colab_type":"code","colab":{}},"source":["def backward_step(dA, layer_cache, activation,weight_type):\n","    \"\"\"\n","    Inputs:\n","        dA: Gradient of the cost function w.r.t. the post activation value A for current layer\n","        layer_cache: A list [A_prev, W, a, b,Z] for current layer, where A_prev is activation of prev layer, W is weight matrix of current layer, and Z is pre-activation value for current layer\n","        activation: Python string denoting the activation function used in the current layer\n","        weight_type = A string denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        dA_prev: Gradient of the cost function w.r.t. the activation (of the previous layer h-1), of same shape as A_prev\n","        dW: Gradient of the cost function w.r.t. W (current layer l), of same shape as W\n","        dbias: Gradient of the cost function w.r.t. bias (current layer l), of same shape as bias\n","        da: Gradient of the cost function w.r.t. a (current layer l), of same shape as a\n","        db: Gradient of the cost function w.r.t. b (current layer l), of same shape as b\n","    \"\"\"\n","    if(weight_type == 'Wavelet'):\n","        A_prev = layer_cache[0]\n","        W = layer_cache[1]\n","        a = layer_cache[2]\n","        b = layer_cache[3]\n","        Z = layer_cache[4] #Actually Z'\n","    elif(weight_type == 'Normal'):\n","        A_prev = layer_cache[0]\n","        W = layer_cache[1]\n","        Z = layer_cache[4]\n","    \n","    m = Z.shape[1]\n","    dZ = act_backward(dA,Z,activation)\n","    if(weight_type == 'Wavelet'):\n","        dZ = dZ / a # dZ = dZ' /a\n","    \n","    dA_prev = np.matmul((W.T),dZ)\n","    dW = (np.matmul(dZ,A_prev.T))/m\n","    dbias = (np.sum(dZ,axis =1,keepdims = True))/m\n","    \n","    flag1 = (np.any(np.isnan(dA_prev)) or np.any(np.isinf(dA_prev)))\n","    flag2 = (np.any(np.isnan(dW)) or np.any(np.isinf(dW)))    \n","    flag3 = (np.any(np.isnan(dbias)) or np.any(np.isinf(dbias)))\n","    flag4 = (np.any(np.isnan(dZ)) or np.any(np.isinf(dZ)))\n","    \n","    if(flag1 or flag2 or flag3 or flag4):\n","        print('A_prev',dA_prev)\n","        print('Z',dZ)\n","        print('W',dW)\n","        print('bias',dbias)\n","        print('A',dA)\n","        print('Wrights',W)\n","        print(weight_type)\n","        raise NameError('Backprop Naned')\n","\n","    if(weight_type == 'Wavelet'):\n","        da = np.sum((dZ * (-1) * Z),axis = 1, keepdims = True)/m # Z-b/a == Z' which is Z in the code (retrieved from cache layer)\n","        db = np.sum((dZ * (-1)),axis = 1, keepdims = True)/m\n","        return dA_prev, dW, dbias,da,db\n","    elif(weight_type == 'Normal'):\n","        return dA_prev, dW, dbias,None, None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQcGcvaj1Xv0","colab_type":"text"},"source":["### Backward Propagation Module \n","* This function computes the gradients of loss function w.r.t. all the weights of the neural network.\n","* To backpropagate through this network, we know that the output is, $A^{[H]} = linear(Z^{[H]})$. Thus compute $ dA^{[h]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[H]}}$. To do so, use this formula:\n","$$ dA^{[h]} = -\\left(Y - A^{[h]}\\right)$$\n","where $ \"/\" \\ denotes \\ elementwise \\ division$\n","* The output of this function is a list ```gradients``` of length $H$, whose each element is a list [$dW^{[h]}, db^{[h]}$].\n","* For the sake of clarity, consider a single-hidden layer NN with input of size 5, hidden layer size = 4 and output layer size =3. For this example, the output ```gradients``` will be [[$dW^{[1]}, db^{[1]}$], [$dW^{[2]}, db^{[2]}$]]."]},{"cell_type":"code","metadata":{"id":"F25e7N_81Xv1","colab_type":"code","colab":{}},"source":["### 1.5 Marks\n","def backward_module(AH, Y, layers_cache, activations,error_function,weight_types):\n","    \"\"\"\n","    Inputs:\n","        AH: Activations of output layer, representing probability vector corresponding to your label predictions, of shape (1, number of examples)\n","        Y: Ground truth \"label\" vector of shape (1, number of examples)\n","        layers_cache: A list (of length H), where each element is a list [A_prev, W, Z], representing the values A_prev (input to the layer), W (Weight matrix of the layer)\n","          and Z (pre-activation value) for layer h. The purpose of cache is to store values that will be needed during backpropagation.\n","        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n","        error_function: mean_square / cross_entropy\n","        weight_types: A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        gradients: A list (of length H), where each element is a list [dW, db], representing the values dW (gradient of cost function w.r.t. W) and  \n","          db (gradient of cost function w.r.t. b)\n","    \"\"\"\n","    gradients = []\n","    H = len(layers_cache) # the number of layers\n","    m = AH.shape[1]\n","    Y = Y.reshape(AH.shape) \n","    if(activations[-1] == \"softmax\" and error_function == \"mean_square\"):\n","        try:\n","            dAH = np.multiply((Y-AH),AH).sum()*AH - np.multiply(Y-AH,AH)\n","        except:\n","            print(AH.shape)\n","            print(np.dot(((Y-AH).T),AH).shape)\n","            print(np.multiply(Y-AH,AH).shape)\n","            print(np.dot(((Y-AH).T),AH) - np.multiply(Y-AH,AH).shape)\n","        assert(dAH.shape == AH.shape)\n","    else:\n","        dAH = -(Y-AH) # Valid for MSE or Cross Entropy with Soft Max\n","        \n","    dA = dAH\n","    for i in reversed(range(H)):\n","        dA_prev, dW, dbias,da,db = backward_step(dA,layers_cache[i],activations[i],weight_types[i])\n","        gradients.insert(0,(dW,dbias,da,db))\n","        dA = dA_prev\n","    return gradients"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlEiE4pY1Xv3","colab_type":"text"},"source":["### Batch Gradient Descent Optimizer\n","Now that we have calculated the gradients of the loss function w.r.t. all the weights of the network, update the weights of the model, using gradient descent: \n","\n","$$ W^{[h]} = W^{[h]} - \\alpha \\text{ } dW^{[h]} $$\n","$$ bias^{[h]} = bias^{[h]} - \\alpha \\text{ } dbias^{[h]} $$\n","$$ a^{[h]} = a^{[h]} - \\alpha \\text{ } da^{[h]} $$\n","$$ b^{[h]} = b^{[h]} - \\alpha \\text{ } db^{[h]} $$\n","\n","where $\\alpha$ is the learning rate.   \n","After computing the updated weights, store them back in the weights list. "]},{"cell_type":"code","metadata":{"id":"JKuvazsP1Xv4","colab_type":"code","colab":{}},"source":["def update_weights(weights, gradients, lr,weight_types):\n","    \"\"\"\n","    Inputs:\n","        weights: A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h (Both Wh and bh are numpy arrays).\n","        gradients: A list (of length H), where each element is a list - [dWh, dbh], representing the gradients of cost function w.r.t. weights matrix and bias vector \n","          of layer h respectively (Both dWh and dbh are numpy arrays).\n","        lr: Learning Rate\n","        weight_types: A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        weights: A list (of length H), containing the updated weights.\n","    \"\"\"\n","    H = len(weights)\n","    for h in range(H):\n","        weights[h][0] -= lr*gradients[h][0]\n","        weights[h][1] -= lr*gradients[h][1]\n","        if(weight_types[h] == 'Wavelet'):\n","            weights[h][2] -= lr*gradients[h][2]\n","            weights[h][3] -= lr*gradients[h][3] \n","            \n","            flag3 = (np.any(np.isnan(weights[h][2])) or np.any(np.isinf(weights[h][2])))\n","            flag4 = (np.any(np.isnan(weights[h][3])) or np.any(np.isinf(weights[h][3])))\n","            if(flag3 or flag4):\n","                print(weights[h][2])\n","                print(weights[h][3])\n","                raise NameError('GOne!!!')\n","                \n","        flag1 = (np.any(np.isnan(weights[h][0])) or np.any(np.isinf(weights[h][0])))\n","        flag2 = (np.any(np.isnan(weights[h][1])) or np.any(np.isinf(weights[h][1])))\n","        if(flag1 or flag2):\n","            print(weights[h][0])\n","            print(weights[h][1])\n","            raise NameError('Gone!!!!')\n","            \n","    return weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0vVngJQn1Xv6","colab_type":"text"},"source":["### Train a H-layer Neural Network:\n","This function performs the following steps:  \n","1. Initialize the weights of the neural network .\n","2. Perform forward propagation on input argument ```X```.\n","3. Compute cross entropy loss function.\n","4. Perform backward propagation to compute gradients of loss function w.r.t. all the weights of the network.\n","5. Use the gradients computed in step 4 to update the weights of the network.\n","6. Append cost's value to the list ```epoch_wise_costs```.\n","7. Print (epoch, cross entropy cost's value) for epoch = 0, 100, 200, ...\n","8. Repeat steps 2-7 for ```epochs```(input argument to below function) number of iterations.\n","9. Return the weights of the neural network.\n"]},{"cell_type":"code","metadata":{"id":"c0RgtSof1Xv7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c9c0d739-32cc-401c-92cf-469efe72f830","executionInfo":{"status":"ok","timestamp":1574403695432,"user_tz":-330,"elapsed":974,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}}},"source":["acs= np.load('/gdrive/My Drive/ADSP Project/init_wavelet.npy')\n","print(acs)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[list([array([[1.21845695e-03, 1.45647275e-03, 3.78607873e-03, 9.24162419e-03,\n","        3.18444485e-03, 9.06751538e-03, 3.77042313e-03, 3.35551116e-03],\n","       [2.63061792e-03, 8.97826260e-03, 7.60984785e-03, 4.09694821e-03,\n","        6.25253835e-03, 9.75474794e-03, 6.74475792e-03, 5.02015004e-03],\n","       [2.64212300e-03, 2.04863002e-03, 2.28828347e-03, 5.99782564e-03,\n","        1.99405178e-03, 8.44800970e-03, 4.12139586e-03, 4.44812341e-03],\n","       [4.16247597e-03, 2.31805599e-04, 8.11118068e-03, 4.60117472e-03,\n","        9.59447931e-03, 7.83811760e-03, 1.69755056e-03, 9.03534407e-03],\n","       [6.73373379e-03, 9.01832113e-03, 2.24892438e-03, 4.32413258e-03,\n","        4.27133248e-03, 8.16343547e-03, 3.46661350e-03, 7.72250725e-03],\n","       [5.39395383e-03, 1.21847138e-03, 9.98058918e-03, 2.56272744e-03,\n","        1.96536464e-04, 5.57794167e-03, 8.83712185e-03, 6.01880088e-03],\n","       [7.96264480e-03, 5.57105980e-03, 8.57098143e-03, 9.44521448e-03,\n","        1.38770568e-03, 2.34435656e-03, 5.31412889e-03, 8.35790886e-03],\n","       [3.00724877e-03, 7.06639045e-03, 4.42307281e-03, 3.38053028e-03,\n","        9.43847985e-03, 1.45403775e-03, 9.34610855e-03, 7.40615264e-03],\n","       [6.50880649e-03, 5.11607459e-03, 3.47809760e-04, 8.63238681e-03,\n","        3.45393413e-03, 3.41259068e-03, 5.33208875e-03, 5.34195439e-03],\n","       [1.70252905e-03, 4.78137687e-03, 9.49475194e-03, 5.14826908e-03,\n","        5.34393638e-03, 5.98023906e-03, 9.50030364e-03, 3.34314597e-03],\n","       [3.73657520e-03, 2.38660168e-03, 5.20786483e-03, 2.81061484e-03,\n","        9.97936774e-03, 1.32529529e-03, 6.53476460e-03, 6.04185541e-05],\n","       [5.65611944e-03, 9.31738707e-03, 7.21103654e-03, 2.99335611e-03,\n","        6.61360727e-03, 1.39902088e-03, 2.86738006e-03, 6.36746504e-03],\n","       [2.56724209e-03, 7.63919330e-03, 7.96056063e-03, 2.16014640e-03,\n","        2.91662735e-03, 1.50112208e-03, 9.20282055e-03, 5.11312229e-03],\n","       [7.14329769e-03, 8.43990473e-03, 9.06023033e-03, 2.60593961e-03,\n","        9.05901550e-03, 4.88378130e-03, 1.87085580e-03, 6.47357156e-03],\n","       [2.29074371e-03, 3.30540883e-03, 6.67971593e-03, 3.13429806e-03,\n","        1.35981269e-03, 7.98236351e-03, 9.07609357e-03, 4.24078393e-03],\n","       [8.45504982e-03, 5.34953550e-03, 3.78688321e-03, 2.18655177e-03,\n","        7.26871307e-03, 2.71730205e-03, 9.87368482e-03, 1.13017642e-03],\n","       [7.48473047e-03, 1.95408544e-03, 7.19779529e-03, 2.80831599e-03,\n","        4.31292730e-03, 7.29618389e-03, 5.37728580e-03, 6.16433596e-03]]), array([[0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.]]), array([[3.31430203],\n","       [3.54366796],\n","       [2.08783338],\n","       [4.23563511],\n","       [0.2180077 ],\n","       [4.13573915],\n","       [4.54501923],\n","       [4.99765703],\n","       [3.68651054],\n","       [3.3781341 ],\n","       [3.18701794],\n","       [1.99058803],\n","       [1.67756301],\n","       [1.93205126],\n","       [0.63472969],\n","       [2.311647  ],\n","       [4.69233694]]), array([[ 3.69416377],\n","       [-5.23822338],\n","       [ 6.93893223],\n","       [ 6.25599969],\n","       [ 7.50508833],\n","       [-2.86551993],\n","       [-4.5713984 ],\n","       [-1.5381027 ],\n","       [ 6.15793152],\n","       [-8.0444555 ],\n","       [ 3.8102914 ],\n","       [-3.41097294],\n","       [-4.738591  ],\n","       [ 5.29081842],\n","       [ 1.14647429],\n","       [ 6.15865406],\n","       [ 9.73886644]])])\n"," list([array([[0.00769287, 0.00067486, 0.00241926, 0.00033121, 0.0009116 ,\n","        0.00894471, 0.00512825, 0.00933523, 0.00399179, 0.00284035,\n","        0.00026197, 0.00214637, 0.00616386, 0.00511838, 0.00552604,\n","        0.00488437, 0.00516337],\n","       [0.00525287, 0.00794885, 0.00589833, 0.0065114 , 0.00776776,\n","        0.00146589, 0.00302121, 0.00754293, 0.00669443, 0.00817214,\n","        0.00773672, 0.00134435, 0.00858061, 0.00289564, 0.0022899 ,\n","        0.00248398, 0.00655182],\n","       [0.006181  , 0.00517227, 0.00358068, 0.0057559 , 0.00108773,\n","        0.00886252, 0.00390763, 0.00611339, 0.00195129, 0.00822473,\n","        0.00436711, 0.00988225, 0.00959797, 0.00438654, 0.00818451,\n","        0.0096428 , 0.0073774 ],\n","       [0.00891237, 0.00366041, 0.00483311, 0.0072117 , 0.00483153,\n","        0.00765122, 0.00283552, 0.00540165, 0.0083707 , 0.00883427,\n","        0.00057965, 0.00306665, 0.00934745, 0.00390586, 0.00791394,\n","        0.00948142, 0.00824001],\n","       [0.00606789, 0.00419686, 0.00648937, 0.00395391, 0.00968938,\n","        0.00779392, 0.00148701, 0.00590286, 0.00187002, 0.00567326,\n","        0.00153323, 0.00351688, 0.00778213, 0.00459566, 0.00703403,\n","        0.00237001, 0.00790711],\n","       [0.00044557, 0.00947916, 0.00314051, 0.00220935, 0.00972826,\n","        0.00498585, 0.00710975, 0.00780791, 0.00947858, 0.00586017,\n","        0.0037058 , 0.00478788, 0.00832659, 0.00744531, 0.00023724,\n","        0.00802662, 0.00168021]]), array([[0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.],\n","       [0.]])])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KzPmsZpl1XwA","colab_type":"code","colab":{}},"source":["def H_layer_NN(X, Y, layers_sizes, activations, epochs, lr, take_bias,error_function,weight_types):\n","    \"\"\"\n","    Inputs:\n","        X: Input data, numpy array of shape (number_of_features_in_input_data, number of examples)\n","        Y: Ground truth \"label\" vector of shape (1, number of examples)\n","        layers_sizes: A list (of length H+1) containing the size (number of neurons) of each layer. Element at index 0 represents input layer size.\n","        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n","        epochs: number of epochs of the training loop\n","        lr: learning rate of the gradient descent update rule\n","        take_bias: Take bias, False -> No Bias\n","        error_function: mean_square / cross_entropy\n","        weight_types: A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        weights - weights learnt by the model (can be used for prediction). A list (of length H), where each element is a list - [Wh, bh], representing the weights matrix and bias vector of layer h.\n","    \"\"\"\n","    epoch_wise_costs = []\n","    AH = None\n","            \n","    if(weight_types[0] == \"Normal\"):\n","        weights = np.load('/gdrive/My Drive/ADSP Project/initial.npy')\n","    elif(weight_types[0] == \"Wavelet\"):\n","        weights = np.load('/gdrive/My Drive/ADSP Project/init_wavelet.npy')\n","        \n","    for epoch in range(epochs):\n","        AH, layers_cache = forward_module(X, weights, activations,take_bias,weight_types)\n","        cost = cost_function(AH, Y,error_function)\n","        gradients = backward_module(AH, Y, layers_cache, activations,error_function,weight_types)\n","        \n","        weights = update_weights(weights, gradients, lr,weight_types)\n","        epoch_wise_costs.append(cost)\n","        if epoch % 100 == 0 or epoch == epochs -1:\n","            print(\"Cost after epoch %i: %f\" %(epoch, cost))\n","            \n","    if(weight_types[0] == \"Normal\"):\n","        plt.plot(np.array(epoch_wise_costs), \"r\")\n","    elif(weight_types[0] == \"Wavelet\"):\n","        plt.plot(np.array(epoch_wise_costs),\"g\")\n","    plt.ylabel('Cost Function')\n","    plt.xlabel('Number of Epochs')\n","    plt.title(\"Training Curve\")\n","\n","    return weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ZO1Q2y81XwE","colab_type":"code","colab":{}},"source":["def predict(X, Y, weights, activations, mode,take_bias, error_function,weight_types):\n","    \"\"\"\n","    Inputs:\n","        X: Input data/examples you would like to predict\n","        Y: Ground truth labels corresponding to input data X\n","        weights: weights of the trained model\n","        activations: A list (of length H), where each element is a Python string representing the activation function to be used in the corresponding layer\n","        mode: Test/Training mode\n","        take_bias: Take bias, False -> No Bias\n","         error_function: mean_square / cross_entropy\n","        weight_types: A list denoting if the activation layer is of Normal/Wavelet type\n","    Outputs:\n","        p: predictions for the given dataset X\n","    \"\"\"\n","    m = X.shape[1]\n","    AH, _ = forward_module(X, weights, activations, take_bias,weight_types)\n","    cost = cost_function(AH, Y,error_function)\n","    p = np.zeros_like(AH)\n","    p[AH == np.amax(AH,axis=0)] = 1\n","    for i in range(p.shape[1]):\n","        while(np.sum(p[:,i]) > 1):\n","            pos_pred = np.argmax(p[:,i])\n","            p[pos_pred,i] = 0\n","    correct = np.sum(np.argmax(p,axis=0) == np.argmax(Y,axis=0))\n","    NN_type = weight_types[0]\n","    print(str(NN_type) +\" Neural Network: \" + mode + \" Cost: \"  + str(cost))\n","    print(str(NN_type) +\" Neural Network: \" + mode + \" Accuracy: \"  + str(correct/m))\n","    return p"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VFPjBuxE1XwH","colab_type":"text"},"source":["### Main Function"]},{"cell_type":"code","metadata":{"id":"RJ_VVAX21XwJ","colab_type":"code","outputId":"8e774d9b-5c01-4d9f-b662-7fa31d8602d4","executionInfo":{"status":"ok","timestamp":1574403733752,"user_tz":-330,"elapsed":32134,"user":{"displayName":"SAKSHAM CONSUL","photoUrl":"","userId":"12844193656018091997"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["layers_sizes = [8, 17, 6]\n","epochs = 10000\n","lr = 0.03\n","TAKE_BIAS = False\n","error_function = \"mean_square\"\n","activations_normal = [\"tanh\",\"linear\"]\n","activations_wavelet = [\"gaussian_wavelet\",\"linear\"]\n","weight_types_NN = [\"Normal\",\"Normal\"]\n","weight_types_WNN = [\"Wavelet\",\"Normal\"]\n","print(\"******************** NORMAL ARTIFICIAL NEURAL NETWORK ********************\")\n","weights_normal = H_layer_NN(X_train, Y_train, layers_sizes, activations_normal, epochs, lr, TAKE_BIAS,error_function,weight_types_NN)\n","print(\"******************** NORMAL ARTIFICIAL NEURAL NETWORK PREDICTION********************\")\n","training_predictions_normal = predict(X_train, Y_train, weights_normal, activations_normal, \"Train\",TAKE_BIAS,error_function,weight_types_NN)\n","# test_predictions_normal = predict(X_test, Y_test, weights_normal, activations_normal, \"Test\",False,weight_types_NN)\n","print(\"******************** WAVELET NEURAL NETWORK ********************\")\n","\n","weights_wavelet = H_layer_NN(X_train, Y_train, layers_sizes, activations_wavelet, epochs, lr, TAKE_BIAS,error_function,weight_types_WNN)\n","print(\"******************** WAVELET NEURAL PREDICTION ********************\")\n","training_predictions_wavelet = predict(X_train, Y_train, weights_wavelet, activations_wavelet, \"Train\",TAKE_BIAS,error_function,weight_types_WNN)\n","# test_predictions_wavelet = predict(X_test, Y_test, weights_wavelet, activations_wavelet, \"Test\",TAKE_BIAS,weight_types_WNN)\n","plt.legend([\"Normal\",\"Wavelet\"])"],"execution_count":21,"outputs":[{"output_type":"stream","text":["******************** NORMAL ARTIFICIAL NEURAL NETWORK ********************\n","Cost after epoch 0: 0.497803\n","Cost after epoch 100: 0.441341\n","Cost after epoch 200: 0.422400\n","Cost after epoch 300: 0.422246\n","Cost after epoch 400: 0.422124\n","Cost after epoch 500: 0.422011\n","Cost after epoch 600: 0.421905\n","Cost after epoch 700: 0.421804\n","Cost after epoch 800: 0.421706\n","Cost after epoch 900: 0.421609\n","Cost after epoch 1000: 0.421511\n","Cost after epoch 1100: 0.421409\n","Cost after epoch 1200: 0.421299\n","Cost after epoch 1300: 0.421178\n","Cost after epoch 1400: 0.421040\n","Cost after epoch 1500: 0.420877\n","Cost after epoch 1600: 0.420679\n","Cost after epoch 1700: 0.420431\n","Cost after epoch 1800: 0.420115\n","Cost after epoch 1900: 0.419707\n","Cost after epoch 2000: 0.419175\n","Cost after epoch 2100: 0.418481\n","Cost after epoch 2200: 0.417583\n","Cost after epoch 2300: 0.416440\n","Cost after epoch 2400: 0.415023\n","Cost after epoch 2500: 0.413327\n","Cost after epoch 2600: 0.411379\n","Cost after epoch 2700: 0.409230\n","Cost after epoch 2800: 0.406942\n","Cost after epoch 2900: 0.404575\n","Cost after epoch 3000: 0.402173\n","Cost after epoch 3100: 0.399771\n","Cost after epoch 3200: 0.397396\n","Cost after epoch 3300: 0.395071\n","Cost after epoch 3400: 0.392812\n","Cost after epoch 3500: 0.390632\n","Cost after epoch 3600: 0.388539\n","Cost after epoch 3700: 0.386537\n","Cost after epoch 3800: 0.384632\n","Cost after epoch 3900: 0.382831\n","Cost after epoch 4000: 0.381141\n","Cost after epoch 4100: 0.379568\n","Cost after epoch 4200: 0.378115\n","Cost after epoch 4300: 0.376777\n","Cost after epoch 4400: 0.375550\n","Cost after epoch 4500: 0.374424\n","Cost after epoch 4600: 0.373391\n","Cost after epoch 4700: 0.372440\n","Cost after epoch 4800: 0.371561\n","Cost after epoch 4900: 0.370746\n","Cost after epoch 5000: 0.369988\n","Cost after epoch 5100: 0.369279\n","Cost after epoch 5200: 0.368613\n","Cost after epoch 5300: 0.367984\n","Cost after epoch 5400: 0.367390\n","Cost after epoch 5500: 0.366825\n","Cost after epoch 5600: 0.366285\n","Cost after epoch 5700: 0.365770\n","Cost after epoch 5800: 0.365274\n","Cost after epoch 5900: 0.364798\n","Cost after epoch 6000: 0.364338\n","Cost after epoch 6100: 0.363892\n","Cost after epoch 6200: 0.363461\n","Cost after epoch 6300: 0.363041\n","Cost after epoch 6400: 0.362632\n","Cost after epoch 6500: 0.362233\n","Cost after epoch 6600: 0.361842\n","Cost after epoch 6700: 0.361460\n","Cost after epoch 6800: 0.361086\n","Cost after epoch 6900: 0.360718\n","Cost after epoch 7000: 0.360356\n","Cost after epoch 7100: 0.360001\n","Cost after epoch 7200: 0.359650\n","Cost after epoch 7300: 0.359305\n","Cost after epoch 7400: 0.358965\n","Cost after epoch 7500: 0.358629\n","Cost after epoch 7600: 0.358298\n","Cost after epoch 7700: 0.357971\n","Cost after epoch 7800: 0.357648\n","Cost after epoch 7900: 0.357330\n","Cost after epoch 8000: 0.357016\n","Cost after epoch 8100: 0.356706\n","Cost after epoch 8200: 0.356400\n","Cost after epoch 8300: 0.356099\n","Cost after epoch 8400: 0.355802\n","Cost after epoch 8500: 0.355510\n","Cost after epoch 8600: 0.355222\n","Cost after epoch 8700: 0.354939\n","Cost after epoch 8800: 0.354660\n","Cost after epoch 8900: 0.354387\n","Cost after epoch 9000: 0.354118\n","Cost after epoch 9100: 0.353853\n","Cost after epoch 9200: 0.353594\n","Cost after epoch 9300: 0.353340\n","Cost after epoch 9400: 0.353090\n","Cost after epoch 9500: 0.352845\n","Cost after epoch 9600: 0.352605\n","Cost after epoch 9700: 0.352370\n","Cost after epoch 9800: 0.352139\n","Cost after epoch 9900: 0.351913\n","Cost after epoch 9999: 0.351694\n","******************** NORMAL ARTIFICIAL NEURAL NETWORK PREDICTION********************\n","Normal Neural Network: Train Cost: 0.35169150499891894\n","Normal Neural Network: Train Accuracy: 0.38095238095238093\n","******************** WAVELET NEURAL NETWORK ********************\n","Cost after epoch 0: 0.499792\n","Cost after epoch 100: 0.416519\n","Cost after epoch 200: 0.416476\n","Cost after epoch 300: 0.416395\n","Cost after epoch 400: 0.416228\n","Cost after epoch 500: 0.415898\n","Cost after epoch 600: 0.415306\n","Cost after epoch 700: 0.414325\n","Cost after epoch 800: 0.412636\n","Cost after epoch 900: 0.409456\n","Cost after epoch 1000: 0.404018\n","Cost after epoch 1100: 0.398305\n","Cost after epoch 1200: 0.394424\n","Cost after epoch 1300: 0.391393\n","Cost after epoch 1400: 0.388796\n","Cost after epoch 1500: 0.386541\n","Cost after epoch 1600: 0.384574\n","Cost after epoch 1700: 0.382851\n","Cost after epoch 1800: 0.381338\n","Cost after epoch 1900: 0.380005\n","Cost after epoch 2000: 0.378829\n","Cost after epoch 2100: 0.377789\n","Cost after epoch 2200: 0.376866\n","Cost after epoch 2300: 0.376045\n","Cost after epoch 2400: 0.375314\n","Cost after epoch 2500: 0.374660\n","Cost after epoch 2600: 0.374074\n","Cost after epoch 2700: 0.373547\n","Cost after epoch 2800: 0.373073\n","Cost after epoch 2900: 0.372644\n","Cost after epoch 3000: 0.372254\n","Cost after epoch 3100: 0.371900\n","Cost after epoch 3200: 0.371576\n","Cost after epoch 3300: 0.371280\n","Cost after epoch 3400: 0.371008\n","Cost after epoch 3500: 0.370757\n","Cost after epoch 3600: 0.370525\n","Cost after epoch 3700: 0.370310\n","Cost after epoch 3800: 0.370110\n","Cost after epoch 3900: 0.369924\n","Cost after epoch 4000: 0.369750\n","Cost after epoch 4100: 0.369586\n","Cost after epoch 4200: 0.369432\n","Cost after epoch 4300: 0.369288\n","Cost after epoch 4400: 0.369151\n","Cost after epoch 4500: 0.369021\n","Cost after epoch 4600: 0.368899\n","Cost after epoch 4700: 0.368782\n","Cost after epoch 4800: 0.368671\n","Cost after epoch 4900: 0.368565\n","Cost after epoch 5000: 0.368463\n","Cost after epoch 5100: 0.368366\n","Cost after epoch 5200: 0.368273\n","Cost after epoch 5300: 0.368184\n","Cost after epoch 5400: 0.368098\n","Cost after epoch 5500: 0.368015\n","Cost after epoch 5600: 0.367935\n","Cost after epoch 5700: 0.367858\n","Cost after epoch 5800: 0.367784\n","Cost after epoch 5900: 0.367712\n","Cost after epoch 6000: 0.367642\n","Cost after epoch 6100: 0.367574\n","Cost after epoch 6200: 0.367508\n","Cost after epoch 6300: 0.367444\n","Cost after epoch 6400: 0.367382\n","Cost after epoch 6500: 0.367322\n","Cost after epoch 6600: 0.367263\n","Cost after epoch 6700: 0.367205\n","Cost after epoch 6800: 0.367149\n","Cost after epoch 6900: 0.367094\n","Cost after epoch 7000: 0.367040\n","Cost after epoch 7100: 0.366988\n","Cost after epoch 7200: 0.366936\n","Cost after epoch 7300: 0.366886\n","Cost after epoch 7400: 0.366836\n","Cost after epoch 7500: 0.366788\n","Cost after epoch 7600: 0.366740\n","Cost after epoch 7700: 0.366694\n","Cost after epoch 7800: 0.366648\n","Cost after epoch 7900: 0.366603\n","Cost after epoch 8000: 0.366558\n","Cost after epoch 8100: 0.366514\n","Cost after epoch 8200: 0.366471\n","Cost after epoch 8300: 0.366429\n","Cost after epoch 8400: 0.366387\n","Cost after epoch 8500: 0.366345\n","Cost after epoch 8600: 0.366305\n","Cost after epoch 8700: 0.366264\n","Cost after epoch 8800: 0.366224\n","Cost after epoch 8900: 0.366185\n","Cost after epoch 9000: 0.366146\n","Cost after epoch 9100: 0.366108\n","Cost after epoch 9200: 0.366069\n","Cost after epoch 9300: 0.366032\n","Cost after epoch 9400: 0.365994\n","Cost after epoch 9500: 0.365957\n","Cost after epoch 9600: 0.365920\n","Cost after epoch 9700: 0.365884\n","Cost after epoch 9800: 0.365848\n","Cost after epoch 9900: 0.365812\n","Cost after epoch 9999: 0.365777\n","******************** WAVELET NEURAL PREDICTION ********************\n","Wavelet Neural Network: Train Cost: 0.3657763758249584\n","Wavelet Neural Network: Train Accuracy: 0.3126984126984127\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f3d220021d0>"]},"metadata":{"tags":[]},"execution_count":21},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"ysdYLP_31Xwc","colab_type":"text"},"source":["### Results"]},{"cell_type":"code","metadata":{"id":"HHgvkXLP1Xwe","colab_type":"code","colab":{}},"source":["training_predictions_normal = predict(X_train, Y_train, weights_normal, activations_normal, \"Train\",TAKE_BIAS,error_function,weight_types_NN)\n","test_predictions_normal = predict(X_test, Y_test, weights_normal, activations_normal, \"Test\",TAKE_BIAS,error_function,weight_types_NN)\n","training_predictions_wavelet = predict(X_train, Y_train, weights_wavelet, activations_wavelet, \"Train\",TAKE_BIAS,error_function,weight_types_WNN)\n","test_predictions_wavelet = predict(X_test, Y_test, weights_wavelet, activations_wavelet, \"Test\",TAKE_BIAS,error_function,weight_types_WNN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xxbrsd7b1Xwm","colab_type":"text"},"source":["Fin."]},{"cell_type":"code","metadata":{"id":"Jlv4hXGe1Xwn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}